{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embeddings from small aligned data to compare translational choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(30) #for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dire ='/Users/yuribizzoni/Downloads/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Europarl corpus in English and Spanish\n",
    "esp = open(dire+\"Translation/B7/epuds-parallel/words/epuds.en-es.es\").read()\n",
    "eng = open(dire+\"Translation/B7/epuds-parallel/words/epuds.en-es.en\").read()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19400792, 17337842)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(esp), len(eng) #length in chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125853, 125853)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#English is the original, Spanish the translation. The two are sentence-aligned.\n",
    "len(esp.split(\"\\n\")), len(eng.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125853 125853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'gustaría',\n",
       " 'agradecerle',\n",
       " 'la',\n",
       " 'oportunidad',\n",
       " 'que',\n",
       " 'me',\n",
       " 'brinda',\n",
       " 'de',\n",
       " 'dirigirme',\n",
       " 'a',\n",
       " 'la',\n",
       " 'cámara',\n",
       " '.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I tokenize each sentence in both corpora\n",
    "palabras =  [nltk.wordpunct_tokenize(fra) for fra in esp.lower().split(\"\\n\")]\n",
    "words = [nltk.wordpunct_tokenize(sen) for sen in eng.lower().split(\"\\n\")]\n",
    "print(len(palabras), len(words))\n",
    "palabras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['giving',\n",
       " 'a',\n",
       " 'la',\n",
       " '.',\n",
       " 'like',\n",
       " '.',\n",
       " 'this',\n",
       " 'you',\n",
       " 'que',\n",
       " 'me',\n",
       " 'president',\n",
       " 'dirigirme',\n",
       " 'house',\n",
       " 'i',\n",
       " 'thank',\n",
       " 'for',\n",
       " 'brinda',\n",
       " 'de',\n",
       " 'should',\n",
       " 'oportunidad',\n",
       " 'address',\n",
       " 'to',\n",
       " 'the',\n",
       " 'la',\n",
       " 'mr',\n",
       " 'agradecerle',\n",
       " 'to',\n",
       " ',',\n",
       " 'opportunity',\n",
       " 'cámara',\n",
       " 'me',\n",
       " 'me',\n",
       " 'gustaría']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I create a single \"sentence\" out of the original and the translation\n",
    "\n",
    "aligned = []\n",
    "for i in range(len(palabras)):\n",
    "    fra = palabras[i]\n",
    "    sen = words[i]\n",
    "    multi = fra+sen\n",
    "    \n",
    "    random.shuffle(multi) #<<< Different shufflings still return good performances\n",
    "    #It seems necessary with this setting to shuffle words' contexts.\n",
    "    #Concatenating the sentences makes the far away context too irrelevant and so the languages stay divided. \n",
    "\n",
    "    aligned.append(multi)\n",
    "    \n",
    "aligned[0] # now Spanish and English are shuffled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finish',\n",
       " 'from',\n",
       " 'de',\n",
       " 'that',\n",
       " '’.',\n",
       " 'odiar',\n",
       " 'él',\n",
       " '‘',\n",
       " 'shaw',\n",
       " 'gustaría',\n",
       " 'ultimate',\n",
       " 'sino',\n",
       " 'una',\n",
       " 'the',\n",
       " 'es',\n",
       " ',',\n",
       " 'no',\n",
       " 'him',\n",
       " 'shaw',\n",
       " 'to',\n",
       " 'no',\n",
       " 'inhumano',\n",
       " 'prójimo',\n",
       " 'hate',\n",
       " 'like',\n",
       " 'is',\n",
       " '».',\n",
       " 'fellow',\n",
       " 'es',\n",
       " 'eso',\n",
       " 'i',\n",
       " 'but',\n",
       " 'of',\n",
       " 'mostrar',\n",
       " 'be',\n",
       " ':',\n",
       " 'not',\n",
       " 'essence',\n",
       " 'to',\n",
       " 'inhumanity',\n",
       " ':',\n",
       " '«',\n",
       " 'por',\n",
       " 'bernard',\n",
       " ',',\n",
       " 'esencia',\n",
       " 'with',\n",
       " 'me',\n",
       " 'george',\n",
       " 'alguno',\n",
       " 'a',\n",
       " 'apathetic',\n",
       " 'al',\n",
       " 'quote',\n",
       " 'cita',\n",
       " 'your',\n",
       " 'pecado',\n",
       " 'the',\n",
       " 'bernard',\n",
       " 'de',\n",
       " 'interés',\n",
       " 'mayor',\n",
       " 'is',\n",
       " 'towards',\n",
       " 'man',\n",
       " 'to',\n",
       " 'should',\n",
       " 'lo',\n",
       " 'george',\n",
       " 'con',\n",
       " 'terminar',\n",
       " 'el',\n",
       " 'sin',\n",
       " 'la']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another shuffled sentence and how it looks like\n",
    "aligned[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.57603712267487, 30.05990216341198)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average length and standard deviation of our concatenated bilingual sentences\n",
    "np.mean([len(s) for s in aligned]), np.std([len(s) for s in aligned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 862 ms, total: 3min\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#I train a model on the average + std (53+30=83) deviation bilingual context of each word.\n",
    "# 300 is the standard w2v size\n",
    "madmod0 = Word2Vec(aligned,window=83,size=300)\n",
    "# This should take around 1 or 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alemania', 0.9596759080886841),\n",
       " ('italy', 0.8487658500671387),\n",
       " ('italia', 0.8454217910766602),\n",
       " ('francia', 0.8358932733535767),\n",
       " ('france', 0.7950721979141235),\n",
       " ('españa', 0.79485684633255),\n",
       " ('spain', 0.7900786399841309),\n",
       " ('belgium', 0.7790611982345581),\n",
       " ('bélgica', 0.7689522504806519),\n",
       " ('suecia', 0.760866641998291)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod0.wv.most_similar(\"germany\") # if it worked, we should see a bilingual country cluster\n",
    "# with Alemania in pole position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So, summing up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. I have to shuffle the data to obtain a working translingual. If I do not shuffle, the spaces keep apart.\n",
    "# This can be probably improved in a new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. If I simply zip the data (\"the\",\"la\",\"house\",\"casa\"), results are better but, it\n",
    "# seems to me, not as good as when i shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Function words and prepositions do not return meaningul clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guerra', 0.9212468862533569),\n",
       " ('terror', 0.7476201057434082),\n",
       " ('genocide', 0.714745044708252),\n",
       " ('genocidio', 0.7048125863075256),\n",
       " ('fría', 0.7038137912750244),\n",
       " ('cold', 0.6748931407928467),\n",
       " ('guerras', 0.6603832244873047),\n",
       " ('brutal', 0.6585128903388977),\n",
       " ('tyranny', 0.6539484262466431),\n",
       " ('bagdad', 0.6538013815879822)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Regarding content words, I get frequent translations and semantically related elements\n",
    "# in both languages.  \n",
    "madmod0.wv.most_similar(\"war\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('voice', 0.9031227827072144),\n",
       " ('sola', 0.527403712272644),\n",
       " ('speak', 0.5132758021354675),\n",
       " ('solidaridad', 0.5124000310897827),\n",
       " ('vision', 0.5023461580276489),\n",
       " ('hable', 0.49363863468170166),\n",
       " ('fortaleza', 0.4901423454284668),\n",
       " ('solidarity', 0.4867943525314331),\n",
       " ('peoples', 0.4831586182117462),\n",
       " ('defended', 0.46939510107040405)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. If a word is (almost) always translated with another word, they should be very close\n",
    "# in the space. \n",
    "madmod0.wv.most_similar(\"voz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('francia', 0.9057453870773315),\n",
       " ('germany', 0.7950721979141235),\n",
       " ('alemania', 0.793376088142395),\n",
       " ('italy', 0.7632391452789307),\n",
       " ('italia', 0.7431871891021729),\n",
       " ('belgium', 0.7328687906265259),\n",
       " ('holland', 0.7214162349700928),\n",
       " ('bélgica', 0.7132459282875061),\n",
       " ('suecia', 0.7081535458564758),\n",
       " ('españa', 0.7032994031906128)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. If a word belongs to a semantically related cluster where most words have a \n",
    "# *consistent translation* in the corpus, such word has many bilingual, near neighbours.\n",
    "madmod0.wv.most_similar(\"france\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solidaridad', 0.9231983423233032),\n",
       " ('peoples', 0.6054810881614685),\n",
       " ('uprising', 0.5656891465187073),\n",
       " ('vecinos', 0.549111008644104),\n",
       " ('sympathy', 0.5339564085006714),\n",
       " ('pueblos', 0.5334646105766296),\n",
       " ('alliance', 0.5277429223060608),\n",
       " ('balkans', 0.5262425541877747),\n",
       " ('inspiración', 0.514620304107666),\n",
       " ('balcanes', 0.5130844712257385)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6. If instead a word does not belong to a group of closely related, consistently \n",
    "# translated concepts, but does have a consistent translation in the corpus,\n",
    "# it tends to have a very close neighbourg followed by a \"void\" (the second nearest is not that near) \n",
    "madmod0.wv.most_similar(\"solidarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('temo', 0.7443530559539795),\n",
       " ('miedo', 0.6412690877914429),\n",
       " ('temor', 0.5605615973472595),\n",
       " ('afraid', 0.5554525852203369),\n",
       " ('temen', 0.5006039142608643),\n",
       " ('feel', 0.4887058734893799),\n",
       " ('ordinary', 0.4820263981819153),\n",
       " ('sienten', 0.471314013004303),\n",
       " ('worry', 0.46651700139045715),\n",
       " ('sensación', 0.46651220321655273)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. If a word is *not* consistently translated, it should display: \n",
    "#1 - various translations, but none very close:\n",
    "madmod0.wv.most_similar(\"fear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sencillamente', 0.46420466899871826),\n",
       " ('impresión', 0.4548487067222595),\n",
       " ('impression', 0.4505397081375122),\n",
       " ('fuera', 0.4258045554161072),\n",
       " ('claim', 0.42454245686531067),\n",
       " ('simply', 0.42331182956695557),\n",
       " ('foolish', 0.40560412406921387),\n",
       " ('wrong', 0.4022558331489563),\n",
       " ('simplemente', 0.4015910029411316),\n",
       " ('puerta', 0.39861059188842773)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#... or: 2 - no good translation at all, if the translation cannot be closed in one \n",
    "# single word for example. The word also appears isolated (closest neighbours are far away)\n",
    "madmod0.wv.most_similar(\"somehow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indecibles', 0.45576515793800354),\n",
       " ('pueblo', 0.4443557560443878),\n",
       " ('helping', 0.43870389461517334),\n",
       " ('población', 0.43117275834083557),\n",
       " ('dictaduras', 0.42869263887405396),\n",
       " ('génova', 0.41671663522720337),\n",
       " ('pueblos', 0.41389721632003784),\n",
       " ('habitantes', 0.4101772904396057),\n",
       " ('fledgling', 0.4068302512168884),\n",
       " ('bi', 0.4042319357395172)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this could be a special way of seeing (1) imbalances in translation\n",
    "madmod0.wv.most_similar(\"gentes\") \n",
    "#gentes does not seem to have a systematic translation in this specific corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('however', 0.8158111572265625),\n",
       "  ('nevertheless', 0.7060055732727051),\n",
       "  ('yet', 0.671391487121582),\n",
       "  ('although', 0.6627480983734131),\n",
       "  ('nonetheless', 0.6286283731460571),\n",
       "  ('though', 0.6247971653938293),\n",
       "  ('whilst', 0.4798395335674286),\n",
       "  ('albeit', 0.4697149097919464),\n",
       "  ('ohio', 0.4690318703651428),\n",
       "  ('while', 0.45984405279159546)],\n",
       " [('de', 0.24182571470737457),\n",
       "  ('jefes', 0.24131551384925842),\n",
       "  ('lanzamiento', 0.2320137917995453),\n",
       "  ('sensibilidades', 0.2230995148420334),\n",
       "  ('dup', 0.2217485010623932),\n",
       "  ('sensitivities', 0.21217766404151917),\n",
       "  ('presidentes', 0.2098350077867508),\n",
       "  ('strengthening', 0.20492029190063477),\n",
       "  ('reclamar', 0.20268955826759338),\n",
       "  ('competences', 0.20201155543327332)])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) strong divergence in polysemy or in function \n",
    "madmod0.wv.most_similar(\"but\"),madmod0.wv.most_similar(\"the\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('palestino', 0.9038184881210327),\n",
       " ('palestinos', 0.895012617111206),\n",
       " ('palestina', 0.8868119716644287),\n",
       " ('israeli', 0.8694062232971191),\n",
       " ('israelí', 0.8532701730728149),\n",
       " ('israelíes', 0.8434242606163025),\n",
       " ('palestinians', 0.824909508228302),\n",
       " ('hamas', 0.814632773399353),\n",
       " ('sharon', 0.8078178763389587),\n",
       " ('hamás', 0.7955443859100342)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(3) contextual similarities beyond a word's translation...\n",
    "madmod0.wv.most_similar(\"palestinian\") \n",
    "#e.g. palestinian is close to israeli too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inhabitants', 0.6230965852737427),\n",
       " ('living', 0.6135353446006775),\n",
       " ('ageing', 0.6059086918830872),\n",
       " ('populations', 0.5475015640258789),\n",
       " ('starvation', 0.5466575622558594),\n",
       " ('viven', 0.5463178157806396),\n",
       " ('vive', 0.5347788333892822),\n",
       " ('envejecimiento', 0.5248936414718628),\n",
       " ('wars', 0.5197626352310181),\n",
       " ('rising', 0.5144315958023071)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#and (4) DISsimilarities between sources and translations. \n",
    "madmod0.wv.most_similar(\"population\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('quisiera', 0.8596170544624329),\n",
       "  ('deseo', 0.7200720310211182),\n",
       "  ('desearía', 0.6660672426223755),\n",
       "  ('querría', 0.6044778823852539),\n",
       "  ('gustaría', 0.5378729104995728),\n",
       "  ('quieren', 0.5127868056297302),\n",
       "  ('quieran', 0.5108271837234497),\n",
       "  ('centrarme', 0.49832069873809814),\n",
       "  ('voy', 0.48674386739730835),\n",
       "  ('desean', 0.4450622498989105)],\n",
       " [('fighting', 0.6652612686157227),\n",
       "  ('islámica', 0.6639626026153564),\n",
       "  ('islamic', 0.6553363800048828),\n",
       "  ('dictatorship', 0.6428117752075195),\n",
       "  ('fundamentalista', 0.6270154118537903),\n",
       "  ('dictadura', 0.6264300346374512),\n",
       "  ('brutal', 0.612619161605835),\n",
       "  ('jihad', 0.6038496494293213),\n",
       "  ('communist', 0.6018915176391602),\n",
       "  ('hatred', 0.5954686403274536)])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words that do not show consistent translations in the space *tend* to cluster monolingually\n",
    "# and to have more entropic clusters\n",
    "madmod0.wv.most_similar(\"quiero\"),madmod0.wv.most_similar(\"struggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('interesante', 0.895222544670105), ('interesantes', 0.6426743268966675), ('slightly', 0.5177820324897766), ('observar', 0.4860764145851135), ('extraordinary', 0.47382843494415283), ('subjects', 0.46645110845565796), ('plantearon', 0.4625413715839386), ('recogen', 0.45739126205444336), ('strange', 0.4562941789627075), ('ligeramente', 0.442565381526947)] \n",
      "\n",
      "[('ciudad', 0.9179441928863525), ('londres', 0.7915592193603516), ('london', 0.7884848117828369), ('condado', 0.7076954245567322), ('inglaterra', 0.7052062749862671), ('manchester', 0.6953339576721191), ('cork', 0.6923949718475342), ('edimburgo', 0.691721498966217), ('circunscripción', 0.6863150000572205), ('england', 0.6812542080879211)] \n",
      "\n",
      "[('successor', 0.7768109440803528), ('sajarov', 0.5914726257324219), ('shamefully', 0.5823409557342529), ('ambassador', 0.5802516937255859), ('robert', 0.5788925886154175), ('gordon', 0.5782333016395569), ('reverend', 0.5698007345199585), ('arzobispo', 0.564502477645874), ('roche', 0.5611132979393005), ('david', 0.5610712766647339)] \n",
      "\n",
      "[('forests', 0.8556815385818481), ('forest', 0.74674391746521), ('fauna', 0.7174651622772217), ('petroleum', 0.7086173892021179), ('tuna', 0.7052651643753052), ('silvestre', 0.7001146674156189), ('bio', 0.6933872103691101), ('atún', 0.6900871992111206), ('forestales', 0.6885117292404175), ('flora', 0.687653660774231)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#words displaying a consistent translation in the space have less entropic distributions,\n",
    "# depending on how systematic the translation might be (see the City-London translation)\n",
    "print(madmod0.wv.most_similar(\"interesting\"),\"\\n\")\n",
    "print(madmod0.wv.most_similar(\"city\"),\"\\n\")\n",
    "print(madmod0.wv.most_similar(\"sucesor\"),\"\\n\")\n",
    "print(madmod0.wv.most_similar(\"bosques\"),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The original being english, we can look into the \"english untranslatables\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struggle\n",
      "[('fighting', 0.7119023203849792), ('violent', 0.673387348651886), ('intimidation', 0.6679258346557617)]\n",
      "\n",
      "\n",
      "pump\n",
      "[('drama', 0.5519206523895264), ('ingresos', 0.550617516040802), ('loans', 0.5242406129837036)]\n",
      "\n",
      "\n",
      "lock\n",
      "[('hacernos', 0.42701491713523865), ('analizan', 0.4033764898777008), ('ansiosos', 0.38876837491989136)]\n",
      "\n",
      "\n",
      "realise\n",
      "[('recognise', 0.5132564902305603), ('feel', 0.4402899742126465), ('show', 0.4151257276535034)]\n",
      "\n",
      "\n",
      "tip\n",
      "[('iceberg', 0.8228524923324585), ('conserva', 0.6419652700424194), ('contributes', 0.6406188607215881)]\n",
      "\n",
      "\n",
      "type\n",
      "[('depósitos', 0.5319403409957886), ('sort', 0.4850086569786072), ('kind', 0.47376930713653564)]\n",
      "\n",
      "\n",
      "stand\n",
      "[('defender', 0.5692262649536133), ('defend', 0.5606892704963684), ('send', 0.5229263305664062)]\n",
      "\n",
      "\n",
      "spam\n",
      "[('imperdonable', 0.6993660926818848), ('hija', 0.6965476274490356), ('fun', 0.6959306001663208)]\n",
      "\n",
      "\n",
      "insight\n",
      "[('maria', 0.5881803035736084), ('apreciación', 0.5480276346206665), ('marxist', 0.5433263182640076)]\n",
      "\n",
      "\n",
      "weekend\n",
      "[('semana', 0.6984114646911621), ('week', 0.6556812524795532), ('ababa', 0.6451869606971741)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From a random site, a small list of palabras ingles sin traducion. \n",
    "sin_traducion = \"\"\"struggle pump lock realise tip type stand spam insight weekend\"\"\".split()\n",
    "for w in sin_traducion:\n",
    "    print(w)\n",
    "    print(madmod0.wv.most_similar(w,topn=3))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none of them has a close neighbour in the other language, which is the behaviour we expect\n",
    "# for elements without a systematic translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mujer', 0.7455636858940125), ('hijo', 0.6782130002975464), ('padre', 0.6462219953536987), ('murió', 0.6421608924865723), ('detenida', 0.6364172697067261), ('soldado', 0.6324895620346069), ('murieron', 0.6123976707458496), ('escuela', 0.6115037798881531), ('prisión', 0.6111001372337341), ('periodista', 0.6093836426734924)]\n",
      "[('hombre', 0.7463650703430176), ('profesor', 0.5815767049789429), ('hijo', 0.5586555600166321), ('niño', 0.5477820038795471), ('islam', 0.5423353910446167), ('padre', 0.5271287560462952), ('tibetano', 0.5224722623825073), ('amigo', 0.5223640203475952), ('joven', 0.5094565749168396), ('persona', 0.5087794661521912)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('triste', 0.6450665593147278),\n",
       " ('historia', 0.5610065460205078),\n",
       " ('walked', 0.5431089997291565),\n",
       " ('guerras', 0.5388035178184509),\n",
       " ('beautiful', 0.5343602299690247),\n",
       " ('terrible', 0.5209183692932129),\n",
       " ('espectáculo', 0.5177900195121765),\n",
       " ('wars', 0.5154507756233215),\n",
       " ('empire', 0.5117136240005493),\n",
       " ('history', 0.5116448998451233)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can even perform Mikolov's man:woman = king:queen kind of operations! \n",
    "\n",
    "#woman:man = x:hombre\n",
    "#man:woman = x:mujer\n",
    "#sad:happy = x:feliz\n",
    "\n",
    "#woman-man+hombre, man-woman+mujer, and so forth\n",
    "print(madmod0.wv.most_similar(positive=[\"woman\",\"hombre\"],negative=[\"man\"]))\n",
    "# woman is to man as hombre is to ?\n",
    "print(madmod0.wv.most_similar(positive=[\"man\",\"mujer\"],negative=[\"woman\"]))\n",
    "madmod0.wv.most_similar(positive=[\"sad\",\"feliz\"],negative=[\"happy\"])\n",
    "# sad is to happy as feliz is to ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('podemos', 0.9999895),\n",
       "  ('puede', 1.05047075e-05),\n",
       "  ('pueden', 1.4487546e-10)],\n",
       " [('sabemos', 1.0), ('conocemos', 5.5914613e-12), ('sé', 3.209026e-12)],\n",
       " [('espero', 1.0), ('confío', 3.298171e-13), ('hope', 1.5322472e-14)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we can somtimes sum words and find the translation of their sum!\n",
    "# For example, the closest word to we+can is podemos, the closest to i+hope is espero etc.\n",
    "\n",
    "madmod0.predict_output_word([\"we\",\"can\"],topn=3),madmod0.predict_output_word([\"we\",\"know\"],topn=3),madmod0.predict_output_word([\"i\",\"hope\"],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is more or less it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I added a long Appendix about variations on this idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Anoher possibility is to mark every word's context with the id of the sentence it is occurring in.\n",
    "alignedO = []\n",
    "for i in range(len(palabras)):\n",
    "    fra = [p for p in palabras[i]]\n",
    "    sen = [w for w in words[i]]#\n",
    "    \n",
    "    multi = fra+sen\n",
    "    fatica =  []#\n",
    "    for w in multi:\n",
    "        fatica.append(w)\n",
    "        fatica.append(str(i))\n",
    "    #random.shuffle(fatica)\n",
    "    alignedO.append(fatica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quiero',\n",
       " '20',\n",
       " 'desearle',\n",
       " '20',\n",
       " ',',\n",
       " '20',\n",
       " 'en',\n",
       " '20',\n",
       " 'nombre',\n",
       " '20',\n",
       " 'del',\n",
       " '20',\n",
       " 'parlamento',\n",
       " '20',\n",
       " ',',\n",
       " '20',\n",
       " 'el',\n",
       " '20',\n",
       " 'mayor',\n",
       " '20',\n",
       " 'de',\n",
       " '20',\n",
       " 'los',\n",
       " '20',\n",
       " 'éxitos',\n",
       " '20',\n",
       " 'durante',\n",
       " '20',\n",
       " 'su',\n",
       " '20',\n",
       " 'mandato',\n",
       " '20',\n",
       " '.',\n",
       " '20',\n",
       " 'i',\n",
       " '20',\n",
       " 'want',\n",
       " '20',\n",
       " 'to',\n",
       " '20',\n",
       " 'wish',\n",
       " '20',\n",
       " 'you',\n",
       " '20',\n",
       " 'every',\n",
       " '20',\n",
       " 'success',\n",
       " '20',\n",
       " ',',\n",
       " '20',\n",
       " 'on',\n",
       " '20',\n",
       " 'behalf',\n",
       " '20',\n",
       " 'of',\n",
       " '20',\n",
       " 'parliament',\n",
       " '20',\n",
       " ',',\n",
       " '20',\n",
       " 'during',\n",
       " '20',\n",
       " 'your',\n",
       " '20',\n",
       " 'term',\n",
       " '20',\n",
       " 'of',\n",
       " '20',\n",
       " 'office',\n",
       " '20',\n",
       " '.',\n",
       " '20']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so a sentence looks like this:\n",
    "alignedO[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I train the model on the smallest context: words occurring in the same set of sentences should thus have the EXACT kind of context (the same set of ids)\n",
    "madmod1= Word2Vec(alignedO[:],window=1,size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('britain', 0.9982935190200806), ('experience', 0.9982931613922119), ('incluido', 0.9982456564903259)]\n",
      "[('alemania', 0.9471361637115479), ('italy', 0.8497835993766785), ('italia', 0.8462876081466675)]\n"
     ]
    }
   ],
   "source": [
    "#This system works very poorly for any content word\n",
    "w = \"germany\"\n",
    "print(madmod1.wv.most_similar(w, topn=3))\n",
    "print(madmod0.wv.most_similar(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('and', 0.924795925617218),\n",
       "  ('de', 0.828447699546814),\n",
       "  ('of', 0.8263126611709595),\n",
       "  ('the', 0.7952296137809753),\n",
       "  ('los', 0.7800973653793335),\n",
       "  ('las', 0.765472412109375),\n",
       "  ('la', 0.7263686656951904),\n",
       "  ('in', 0.6723071932792664),\n",
       "  ('en', 0.6377344131469727),\n",
       "  ('.', 0.6255273818969727)],\n",
       " [('e', 0.4978914260864258),\n",
       "  ('arts', 0.20701898634433746),\n",
       "  ('implicarnos', 0.20574888586997986),\n",
       "  ('societies', 0.18744277954101562),\n",
       "  ('desechar', 0.186610609292984),\n",
       "  ('prosperity', 0.18592122197151184),\n",
       "  ('ponerlos', 0.1841798722743988),\n",
       "  ('óptimos', 0.1799958199262619),\n",
       "  ('compaginar', 0.17966225743293762),\n",
       "  ('owed', 0.17411628365516663)])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but it is working for function words and prepositions!!\n",
    "madmod1.wv.most_similar(\"y\"), madmod0.wv.most_similar(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 0.8716167211532593),\n",
       " ('.', 0.7861998677253723),\n",
       " ('we', 0.7296077609062195),\n",
       " ('dwelling', 0.7217196822166443),\n",
       " ('griega', 0.6852811574935913),\n",
       " ('malliori', 0.6851369142532349),\n",
       " ('will', 0.678860068321228),\n",
       " ('que', 0.6760795712471008),\n",
       " ('have', 0.671581506729126),\n",
       " ('that', 0.671463668346405)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod1.wv.most_similar(\"to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPENDIX B: SAME IN GERMAN\n",
    "deu = open(dire+\"Translation/B7/epuds-parallel/words/epuds.en-de.de\").read()\n",
    "eng = open(dire+\"Translation/B7/epuds-parallel/words/epuds.en-de.en\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137814 137814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['herr',\n",
       " 'präsident',\n",
       " ',',\n",
       " 'vielen',\n",
       " 'dank',\n",
       " ',',\n",
       " 'daß',\n",
       " 'sie',\n",
       " 'mir',\n",
       " 'gelegenheit',\n",
       " 'geben',\n",
       " ',',\n",
       " 'mich',\n",
       " 'an',\n",
       " 'dieses',\n",
       " 'hohe',\n",
       " 'haus',\n",
       " 'zu',\n",
       " 'wenden',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrt =  [nltk.wordpunct_tokenize(se) for se in deu.lower().split(\"\\n\")]\n",
    "words = [nltk.wordpunct_tokenize(sen) for sen in eng.lower().split(\"\\n\")]\n",
    "print(len(wrt), len(words))\n",
    "wrt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr president , i should like to thank you for giving me this opportunity to address the house .\n",
      "herr präsident , vielen dank , daß sie mir gelegenheit geben , mich an dieses hohe haus zu wenden .\n",
      "\n",
      "\n",
      "i want to register my protest at the lack of facilities for me , as a deputy within this chamber .\n",
      "ich möchte meinen protest über mangelhafte einrichtungen für mich als mitglied dieses parlaments zum ausdruck bringen .\n",
      "\n",
      "\n",
      "two and a half years ago , i went to the authorities who were dealing with the building of this new structure and informed them of my requirements as a person with a disability within the chamber .\n",
      "vor zweieinhalb jahren wandte ich mich an die mit der errichtung dieses neuen gebäudes befaßten stellen und informierte sie über meine anforderungen als behinderter abgeordneter .\n",
      "\n",
      "\n",
      "i was assured at that stage that every facility would be made available .\n",
      "damals wurde mir versichert , daß alle voraussetzungen gegeben sein würden .\n",
      "\n",
      "\n",
      "i arrived here last month and spoke with the architects and builders with regard to the seating arrangements within the hemicycle .\n",
      "vergangenen monat kam ich hier an und sprach mit den architekten und bauleuten über die anordnung der sitze im plenarsaal .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    sen = words[i]\n",
    "    print(\" \".join(sen))\n",
    "    print(\" \".join(wrt[i]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " ',',\n",
       " 'the',\n",
       " 'like',\n",
       " 'to',\n",
       " 'an',\n",
       " 'i',\n",
       " '.',\n",
       " 'daß',\n",
       " 'president',\n",
       " 'dieses',\n",
       " 'giving',\n",
       " 'gelegenheit',\n",
       " 'vielen',\n",
       " 'hohe',\n",
       " 'to',\n",
       " 'geben',\n",
       " 'thank',\n",
       " 'house',\n",
       " 'zu',\n",
       " ',',\n",
       " 'haus',\n",
       " 'sie',\n",
       " 'mir',\n",
       " 'me',\n",
       " 'wenden',\n",
       " 'opportunity',\n",
       " 'mich',\n",
       " 'you',\n",
       " 'mr',\n",
       " ',',\n",
       " ',',\n",
       " 'for',\n",
       " 'dank',\n",
       " 'address',\n",
       " 'this',\n",
       " 'should',\n",
       " 'herr',\n",
       " 'präsident']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned = []\n",
    "for i in range(len(wrt)):\n",
    "    se = wrt[i]\n",
    "    sen = words[i]\n",
    "    multi = se+sen\n",
    "    \n",
    "    random.shuffle(multi) \n",
    "    aligned.append(multi)\n",
    "    \n",
    "aligned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51.39989406011, 31.879100414691504)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(s) for s in aligned]), np.std([len(s) for s in aligned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I train a model on the average + std deviation context. So its context is bilingual.\n",
    "madmod_deu = Word2Vec(aligned,window=84,size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deutschland', 0.9349461197853088),\n",
       " ('frankreich', 0.844580888748169),\n",
       " ('france', 0.8314818143844604)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"germany\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('force', 0.6560993790626526),\n",
       " ('inkrafttreten', 0.5913261771202087),\n",
       " ('treten', 0.5754548907279968),\n",
       " ('tritt', 0.5448029041290283),\n",
       " ('ratify', 0.5393043756484985),\n",
       " ('ratifizieren', 0.5074322819709778),\n",
       " ('cartegena', 0.4983678162097931),\n",
       " ('verlängerung', 0.49187955260276794),\n",
       " ('protocol', 0.4881513714790344),\n",
       " ('abgeschlossen', 0.48623502254486084)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"kraft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('glücklich', 0.5722105503082275),\n",
       " ('grateful', 0.5053730010986328),\n",
       " ('freude', 0.5035439133644104),\n",
       " ('froh', 0.47786515951156616),\n",
       " ('erfreut', 0.47060608863830566),\n",
       " ('helpful', 0.4672287106513977),\n",
       " ('zufrieden', 0.46700045466423035),\n",
       " ('zusammengearbeitet', 0.46011000871658325),\n",
       " ('dankbar', 0.45920276641845703),\n",
       " ('speaking', 0.4589686393737793)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('murder', 0.9396565556526184),\n",
       " ('brutale', 0.825391948223114),\n",
       " ('murdered', 0.820722222328186),\n",
       " ('brutal', 0.8183834552764893),\n",
       " ('ermordung', 0.8165302872657776),\n",
       " ('ermordet', 0.8146328926086426),\n",
       " ('killings', 0.8130627870559692),\n",
       " ('camp', 0.8076316118240356),\n",
       " ('bombing', 0.8040317296981812),\n",
       " ('zivilisten', 0.8013164401054382)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"mord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('commissions', 0.3650393486022949),\n",
       " ('likewise', 0.34637245535850525),\n",
       " ('including', 0.3458613157272339),\n",
       " ('applies', 0.33967074751853943),\n",
       " ('worüber', 0.33508801460266113),\n",
       " ('arusha', 0.33462971448898315),\n",
       " ('welche', 0.3186159133911133),\n",
       " ('welchen', 0.31593114137649536),\n",
       " ('accumulated', 0.31573671102523804),\n",
       " ('jene', 0.3134268522262573)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no near neighs\n",
    "madmod_deu.wv.most_similar(\"wozu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kann', 0.691245973110199),\n",
       " ('dürfen', 0.6539732217788696),\n",
       " ('muß', 0.6143087148666382),\n",
       " ('könnte', 0.5920522212982178),\n",
       " ('muss', 0.5890470743179321),\n",
       " ('sollte', 0.5552568435668945),\n",
       " ('können', 0.5299324989318848),\n",
       " ('dürfe', 0.5298604965209961),\n",
       " ('müsse', 0.5247164964675903),\n",
       " ('soll', 0.5089102983474731)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no near english neighs\n",
    "madmod_deu.wv.most_similar(\"darf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('falls', 0.7583459615707397),\n",
       " ('sofern', 0.6681891083717346),\n",
       " ('wann', 0.6511678695678711),\n",
       " ('sobald', 0.48729443550109863),\n",
       " ('then', 0.4788084924221039),\n",
       " ('obwohl', 0.4600425660610199),\n",
       " ('comes', 0.44291406869888306),\n",
       " ('aufseiten', 0.4087792634963989),\n",
       " ('solange', 0.40547245740890503),\n",
       " ('ob', 0.39352846145629883)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no near german neighs\n",
    "madmod_deu.wv.most_similar(\"wenn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('questions', 0.713936984539032),\n",
       " ('themen', 0.6174116134643555),\n",
       " ('issues', 0.5843456983566284),\n",
       " ('beantworten', 0.5346881151199341),\n",
       " ('beantwortet', 0.5341669321060181),\n",
       " ('matters', 0.5292607545852661),\n",
       " ('anfragen', 0.5273733735084534),\n",
       " ('answered', 0.5213537812232971),\n",
       " ('aufgeworfenen', 0.5114202499389648),\n",
       " ('frage', 0.5085440874099731),\n",
       " ('raised', 0.4873928427696228),\n",
       " ('schlüsselfragen', 0.48508137464523315),\n",
       " ('arise', 0.48319876194000244),\n",
       " ('raises', 0.47569799423217773),\n",
       " ('answers', 0.46668359637260437),\n",
       " ('klären', 0.46573853492736816),\n",
       " ('klärung', 0.46054574847221375),\n",
       " ('addressed', 0.4575481414794922),\n",
       " ('question', 0.45689529180526733),\n",
       " ('aufgeworfen', 0.4535149335861206)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"fragen\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('freiheit', 0.8800723552703857),\n",
       "  ('expression', 0.808447003364563),\n",
       "  ('meinungsfreiheit', 0.7776405215263367),\n",
       "  ('meinungsäußerung', 0.720323920249939),\n",
       "  ('freedoms', 0.6940377354621887),\n",
       "  ('religionsfreiheit', 0.6920214891433716),\n",
       "  ('liberty', 0.6800886988639832),\n",
       "  ('gerechtigkeit', 0.671705961227417),\n",
       "  ('freiheiten', 0.6646568179130554),\n",
       "  ('gleichheit', 0.6554666757583618)],\n",
       " [('freiheit', 0.8770506381988525),\n",
       "  ('expression', 0.7552639842033386),\n",
       "  ('meinungsäußerung', 0.7279289960861206),\n",
       "  ('meinungsfreiheit', 0.7247045040130615),\n",
       "  ('freedoms', 0.7050155401229858),\n",
       "  ('religiösen', 0.6755955219268799),\n",
       "  ('pressefreiheit', 0.6700924038887024),\n",
       "  ('gerechtigkeit', 0.6672602891921997),\n",
       "  ('liberty', 0.6613081693649292),\n",
       "  ('religionsfreiheit', 0.6567243337631226)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"freedom\"\n",
    "madmod_deu.wv.most_similar(w), madmod0.wv.most_similar(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pressefreiheit', 0.7436741590499878),\n",
       " ('meinungsfreiheit', 0.741266131401062),\n",
       " ('expression', 0.731926441192627),\n",
       " ('religionsfreiheit', 0.7109665870666504),\n",
       " ('journalists', 0.7109071016311646),\n",
       " ('presse', 0.6837912201881409),\n",
       " ('inhaftierung', 0.6783128976821899),\n",
       " ('journalisten', 0.6756174564361572),\n",
       " ('redefreiheit', 0.6665547490119934),\n",
       " ('freiheit', 0.654988169670105)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar([\"freedom\",\"press\"]) #concepts at the intersection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expression', 0.7611088752746582),\n",
       " ('meinungsfreiheit', 0.7404330372810364),\n",
       " ('religious', 0.7004653215408325),\n",
       " ('folter', 0.6977810859680176),\n",
       " ('freedom', 0.6920214891433716),\n",
       " ('degrading', 0.6678593158721924),\n",
       " ('torture', 0.6671411395072937),\n",
       " ('minorities', 0.648648202419281),\n",
       " ('religiösen', 0.6398242712020874),\n",
       " ('repression', 0.635739803314209)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar([\"religionsfreiheit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delighted', 0.6927938461303711),\n",
       " ('pleased', 0.6917699575424194),\n",
       " ('froh', 0.685878336429596),\n",
       " ('enttäuscht', 0.667832612991333),\n",
       " ('glad', 0.6597051620483398),\n",
       " ('disappointed', 0.6068927049636841),\n",
       " ('freue', 0.5639588832855225),\n",
       " ('freut', 0.5476800203323364),\n",
       " ('gefreut', 0.5431020855903625),\n",
       " ('ermutigend', 0.5235954523086548)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"erfreut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arbeitslosigkeit', 0.908473014831543),\n",
       " ('arbeitslosenquote', 0.6700712442398071),\n",
       " ('produktivität', 0.6622342467308044),\n",
       " ('wachstum', 0.6487407684326172),\n",
       " ('productivity', 0.6370849609375),\n",
       " ('jobs', 0.6321849226951599),\n",
       " ('growth', 0.6256743669509888),\n",
       " ('arbeitsplätze', 0.6215499043464661),\n",
       " ('living', 0.6189567446708679),\n",
       " ('wohlstand', 0.596534252166748)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"unemployment\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERPRETING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting was a bit complicated to pre-process due to the format of the data. I created a txt file with aligned english-german interpreting transcripts and here I will just use that as my source corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     <s id=\"1:7\">and we need a little bit of honesty about that</s>    <s id=\"1:7\">ich glaub da bräuchten wir doch n bisschen mehr Ehrlichkeit</s>\n",
      "['<s', 'id=\"1:7\">and', 'we', 'need', 'a', 'little', 'bit', 'of', 'honesty', 'about', 'that</s>', '<s', 'id=\"1:7\">ich', 'glaub', 'da', 'bräuchten', 'wir', 'doch', 'n', 'bisschen', 'mehr', 'Ehrlichkeit</s>']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "f = codecs.open(\"English-German_aligned_interpreting.txt\",\"r\",\"utf8\")#.read()\n",
    "# here I will use it as it is, ofc you can remove </s> and similar signs before starting\n",
    "mydata = f.read()\n",
    "doubles = mydata.split(\"\\n#\\n\")\n",
    "len(doubles)\n",
    "print(doubles[100])\n",
    "print(doubles[100].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "tokd_alig = []\n",
    "for d in doubles:\n",
    "    magic = nltk.wordpunct_tokenize(d.lower())\n",
    "    random.shuffle(magic)\n",
    "    tokd_alig.append(magic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3397, 47.97438916691198, 30.19486692753781)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokd_alig), np.mean([len(e) for e in tokd_alig]), np.std([len(e) for e in tokd_alig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 265 ms, total: 26.3 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "interp_model = Word2Vec(tokd_alig,window=77,size=300, min_count=1, negative=10, iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spanien', 0.9892104864120483),\n",
       " ('beneficiaries', 0.9067434668540955),\n",
       " ('italy', 0.9027279615402222),\n",
       " ('italien', 0.9006606340408325),\n",
       " ('hauptempfänger', 0.8998532891273499),\n",
       " ('bethe', 0.8667525053024292),\n",
       " ('alte', 0.8648932576179504),\n",
       " ('died', 0.8122330904006958),\n",
       " ('pflegeheimen', 0.8045418858528137),\n",
       " ('residential', 0.8031773567199707)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp_model.wv.most_similar(\"spain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3397"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokd_alig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568230\n"
     ]
    }
   ],
   "source": [
    "tokn = 0\n",
    "for tokd in tokd_alig:\n",
    "    tokn+=sum([len(w) for w in tokd])\n",
    "print(tokn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can make a down-sampled model trained on a translation corpus as small as an interpreting corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(aligned)\n",
    "downsampled_model = Word2Vec(aligned[:len(tokd_alig)],window=80,size=300, min_count=1, negative=10, iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('deutschland', 0.9487962126731873),\n",
       "  ('spain', 0.8551216721534729),\n",
       "  ('frankreich', 0.8506844639778137),\n",
       "  ('france', 0.8486232757568359),\n",
       "  ('belgien', 0.83827805519104)],\n",
       " [('deutschland', 0.8962137699127197),\n",
       "  ('unemployment', 0.8538498878479004),\n",
       "  ('arbeitslosenquoten', 0.8228492736816406),\n",
       "  ('population', 0.7792320251464844),\n",
       "  ('zivilisten', 0.7768558859825134)],\n",
       " [('deutschland', 0.9668587446212769),\n",
       "  ('politically', 0.8618838787078857),\n",
       "  ('politisch', 0.8386322855949402),\n",
       "  ('tragbar', 0.8146785497665405),\n",
       "  ('verbuchen', 0.6270592212677002)])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wort,n = \"germany\",5 #friends,\n",
    "madmod_deu.wv.most_similar(wort, topn=n), downsampled_model.wv.most_similar(wort,topn=n), interp_model.wv.most_similar(wort,topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18114, 10524)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some basic analysis\n",
    "len(downsampled_model.wv.vocab), len(interp_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from here on it is possible to compare the spaces' inner distances, avg closest neighbour and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3443\n"
     ]
    }
   ],
   "source": [
    "smaller_voc = [k for k in downsampled_model.wv.vocab if k in interp_model.wv.vocab]\n",
    "print(len(smaller_voc))\n",
    "smaller_voc = smaller_voc[:]\n",
    "wow = [downsampled_model.wv.distances(w) for w in smaller_voc]\n",
    "wow2 = [interp_model.wv.distances(w) for w in smaller_voc]\n",
    "#hm2 = erste_model.wv.distances(wort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-1d01ea60861b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwow3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmadmod_deu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmaller_voc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-1d01ea60861b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwow3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmadmod_deu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmaller_voc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mdistances\u001b[0;34m(self, word_or_vector, other_words)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mother_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mother_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mother_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mcosine_similarities\u001b[0;34m(vector_1, vectors_all)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \"\"\"\n\u001b[1;32m    756\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mall_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mdot_products\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot_products\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mall_norms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mord\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m             \u001b[0;31m# special case for speedup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2507\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wow3 = [madmod_deu.wv.distances(w) for w in smaller_voc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.mean(wow), np.std(wow), 1-np.mean(wow2), np.std(wow2), 1-np.mean(wow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.mean(wow), np.std(wow), 1-np.mean(wow2), np.std(wow2)#, 1-np.mean(wow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearest neighbour avg distance, second nearest neighbour average distance.\n",
    "nearests_downsampled = [downsampled_model.wv.most_similar(w, topn=10) for w in smaller_voc[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearests_erste = [erste_model.wv.most_similar(w, topn=10) for w in smaller_voc[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearests\n",
    "first_down = [e[0][1] for e in nearests_downsampled[:]]\n",
    "first_ers = [e[0][1] for e in nearests_erste[:]]\n",
    "np.mean(first_down), np.std(first_down), np.mean(first_ers), np.std(first_ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_down = [e[-1][1] for e in nearests_downsampled]\n",
    "second_ers = [e[-1][1] for e in nearests_erste]\n",
    "np.mean(second_down), np.mean(second_ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_down = [sum([w[1] for w in e]) for e in nearests_downsampled]\n",
    "second_ers = [sum([w[1] for w in e]) for e in nearests_erste]\n",
    "np.mean(second_down), np.mean(second_ers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('irgendwie', 0.8418327569961548),\n",
       "  ('anhängen', 0.833200216293335),\n",
       "  ('pollute', 0.8023205995559692),\n",
       "  ('verschmutzt', 0.7767529487609863),\n",
       "  ('beauftragter', 0.7750450372695923),\n",
       "  ('friedens', 0.7689111232757568),\n",
       "  ('einreden', 0.7673659324645996),\n",
       "  ('island', 0.7669362425804138),\n",
       "  ('suggesting', 0.7635915875434875),\n",
       "  ('donated', 0.7626611590385437)],\n",
       " [('enjoy', 0.6445189714431763),\n",
       "  ('speaks', 0.6365010738372803),\n",
       "  ('volumes', 0.6305739879608154),\n",
       "  ('historic', 0.5737761855125427),\n",
       "  ('schlachthöfen', 0.5715336799621582),\n",
       "  ('kontaminierte', 0.5693942308425903),\n",
       "  ('vast', 0.5624562501907349),\n",
       "  ('round', 0.5580256581306458),\n",
       "  ('schweine', 0.5536667108535767),\n",
       "  ('leier', 0.5522794127464294)])"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wort = \"somehow\" #friends,\n",
    "downsampled_model.wv.most_similar(wort), erste_model.wv.most_similar(wort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('quickly', 0.722977876663208),\n",
       "  ('angeboten', 0.6809149980545044),\n",
       "  ('gut', 0.6630717515945435),\n",
       "  ('equally', 0.648383378982544),\n",
       "  ('inconvenient', 0.6144195795059204),\n",
       "  ('recognise', 0.6134805083274841),\n",
       "  ('geflügeltes', 0.6119312047958374),\n",
       "  ('wirtschaftswissenschaftler', 0.6112147569656372),\n",
       "  ('staatsform', 0.6111538410186768),\n",
       "  ('zusammenkommen', 0.6096416711807251)],\n",
       " [('quickly', 0.7965059280395508),\n",
       "  ('gelöst', 0.7955560684204102),\n",
       "  ('asap', 0.784021258354187),\n",
       "  ('b', 0.735526978969574),\n",
       "  ('trinkwasser', 0.7343395948410034),\n",
       "  ('clean', 0.7325656414031982),\n",
       "  ('c', 0.7299182415008545),\n",
       "  ('restore', 0.7273295521736145),\n",
       "  ('passieren', 0.7115418910980225),\n",
       "  ('strom', 0.7114201784133911)])"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wort = \"schnell\" #friends,doctors,doctor,politician,gefahr,gespielt,understand, vs should,could,must,will,very,gratulieren,verträge\n",
    "downsampled_model.wv.most_similar(wort), interp_model.wv.most_similar(wort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ausschußvorsitzender', 0.9808642268180847),\n",
       " ('mitberichterstatter', 0.726360559463501),\n",
       " ('schuldenerlässe', 0.7016815543174744),\n",
       " ('willensentscheidung', 0.6965516805648804),\n",
       " ('führerscheinen', 0.6951477527618408),\n",
       " ('vergnaud', 0.6947504878044128),\n",
       " ('versicherungsklima', 0.688271164894104),\n",
       " ('meeresüberwachung', 0.6873326301574707),\n",
       " ('ausblutet', 0.6771560907363892),\n",
       " ('vorbereitungszeit', 0.6764340996742249)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madmod_deu.wv.most_similar(\"sitte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
